{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, sys\n",
    "sys.path.append(\"/home/suman/Sayari/TrP-cage/Noteboks_Trpcage\")\n",
    "#import modules\n",
    "import numpy as np\n",
    "import MDAnalysis as mda\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pyemma\n",
    "import matplotlib.pyplot as plt\n",
    "##custom scripts\n",
    "from runner import Feature\n",
    "from plot2d import plot_free_energy as pfe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## De Shaw TRP-Cage trajectory\n",
    "path = \"/home/suman/Dibyendu/Trp-Cage-folding/DESRES-Trajectory_2JOF-0-protein/2JOF-0-protein/\"\n",
    "files = os.listdir(path)\n",
    "traj_list = []\n",
    "for file in files:\n",
    "    if file.endswith(\".dcd\"):\n",
    "        traj_list.append(path + file)\n",
    "\n",
    "sorted_traj_list = list(np.sort(traj_list))\n",
    "trp_cage = mda.Universe(\"/home/suman/Sayari/TrP-cage/trp_cage.pdb\", sorted_traj_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features...give the important contact pairs that have discriminative powers..\n",
    "ca_dist= np.loadtxt(\"pairwise_dist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you used other features calculate them accordingly and then scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling of data..\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(ca_dist)\n",
    "scaled_dist = scaler.transform(ca_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distances_tica=scaled_dist.reshape(1,1044000,190)\n",
    "tica = pyemma.coordinates.tica(list(Distances_tica[::]), lag=250) #lagtime 50ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_output = tica.get_output()\n",
    "tica_concatenated = np.concatenate(tica_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_concatenated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml # install gpu based hdbscan module cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_distance = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=20000,min_samples=200,\n",
    "                                                  prediction_data=True, gen_min_span_tree=True,cluster_selection_method=\"eom\")\n",
    "clusterer_distance.fit(scaled_dist[::]) # give input feature of your interest in .fit function, stride if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min_Samples selection should be choosen based on dimensionality of input feature, it controls the amount of noise data points.\n",
    "# min_cluster_size is the control parameter for determinimg number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterlabels= clusterer_distance.labels_\n",
    "np.unique(clusterlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Centriod of Largest Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest cluster\n",
    "unique_labels, counts = np.unique(clusterlabels[clusterlabels != -1], return_counts=True)\n",
    "largest_cluster_label = unique_labels[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter points belonging to the largest cluster\n",
    "largest_cluster = scaled_dist[clusterlabels == largest_cluster_label]\n",
    "\n",
    "# Calculate the centroid\n",
    "centroid = np.mean(largest_cluster, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tica_centriod= tica.transform((centroid.reshape(1, 190)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Centriod of all three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centroids and their tICA transformations for each cluster\n",
    "centroids = []\n",
    "tica_centroids = []\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    cluster = scaled_dist[clusterlabels == i]\n",
    "    centroid = np.mean(cluster, axis=0)\n",
    "    centroids.append(centroid)\n",
    "    tica_centroids.append(tica.transform(centroid.reshape(1, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ,ax = plt.subplots(figsize=[10,8])\n",
    "pfe(tica_concatenated[:,0],tica_concatenated[:,1],ax=ax)\n",
    "plt.scatter(np.array(tica_centroids)[0][:,0],np.array(tica_centroids)[0][:,1],s=500 ,c='red',marker='*')\n",
    "plt.scatter(np.array(tica_centroids)[1][:,0],np.array(tica_centroids)[1][:,1],s=500 ,c='red',marker='*')\n",
    "plt.scatter(np.array(tica_centroids)[2][:,0],np.array(tica_centroids)[2][:,1],s=500 ,c='red',marker='*',label='Cluster Centriods')\n",
    "plt.legend(loc='upper left',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN better for finding the multiple small clusters in data\n",
    "# GMM best in separating folded state from the unfolded state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=3, random_state=42,covariance_type='full').fit(scaled_dist[::10])\n",
    "labels_gm = gm.predict(scaled_dist[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "class Hierarchical_Clastering:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def rgb_hex(self, color):\n",
    "        '''converts a (r,g,b) color (either 0-1 or 0-255) to its hex representation.\n",
    "        for ambiguous pure combinations of 0s and 1s e,g, (0,0,1), (1/1/1) is assumed.'''\n",
    "        message='color must be an iterable of length 3.'\n",
    "        assert hasattr(color, '__iter__'), message\n",
    "        assert len(color)==3, message\n",
    "        if all((c <= 1) & (c >= 0) for c in color): color=[int(round(c*255)) for c in color] # in case provided rgb is 0-1\n",
    "        color=tuple(color)\n",
    "        return '#%02x%02x%02x' % color\n",
    "\n",
    "    def get_cluster_colors(self, n_clusters, alpha=0.8, alpha_outliers=0.05):\n",
    "        #my_set_of_20_rgb_colors =\n",
    "        cluster_colors = [\n",
    "            [\n",
    "                np.random.randint(255),\n",
    "                np.random.randint(255),\n",
    "                np.random.randint(255),\n",
    "            ]\n",
    "            for _ in range(100)\n",
    "        ]\n",
    "        cluster_colors = [c+[alpha] for c in cluster_colors]\n",
    "        outlier_color = [0,0,0,alpha_outliers]\n",
    "        return [cluster_colors[i%19] for i in range(n_clusters)] + [outlier_color]\n",
    "\n",
    "    def clusters(self, X, threshold,no_plot = True, method='ward', metric='euclidean', default_color='black'):\n",
    "\n",
    "        # perform hierarchical clustering\n",
    "        Z              = hierarchy.linkage(X.astype(np.float32), method=method, metric=metric)\n",
    "\n",
    "        # get cluster labels\n",
    "        labels         = hierarchy.fcluster(Z, threshold, criterion='distance') - 1\n",
    "        labels_str     = [f\"cluster #{l}: n={c}\\n\" for (l,c) in zip(*np.unique(labels, return_counts=True))]\n",
    "        n_clusters     = len(labels_str)\n",
    "\n",
    "        cluster_colors = [self.rgb_hex(c[:-1]) for c in self.get_cluster_colors(n_clusters, alpha=0.8, alpha_outliers=0.05)]\n",
    "        cluster_colors = [\"blue\" for i in range(n_clusters)]\n",
    "        #cluster_colors = [\"blue\", \"mediumorchid\", \"green\"] # choose color accordingly\n",
    "\n",
    "        cluster_colors_array = [cluster_colors[l] for l in labels]\n",
    "        link_cols = {}\n",
    "        for i, i12 in enumerate(Z[:,:2].astype(int)):\n",
    "            c1, c2 = (link_cols[x] if x > len(Z) else cluster_colors_array[x] for x in i12)\n",
    "            link_cols[i+1+len(Z)] = c1 if c1 == c2 else 'k'\n",
    "\n",
    "        # plot dendrogram with colored clusters\n",
    "        if no_plot:\n",
    "            self._extracted_from_clusters_20(threshold)\n",
    "        # plot dendrogram based on clustering results\n",
    "        dend = hierarchy.dendrogram(\n",
    "            Z,\n",
    "            no_plot = not no_plot,\n",
    "            labels = labels,\n",
    "            color_threshold=threshold,\n",
    "            truncate_mode = 'level',\n",
    "            p = 5,\n",
    "            show_leaf_counts = True,\n",
    "            leaf_rotation=90,\n",
    "            leaf_font_size=10,\n",
    "            show_contracted=False,\n",
    "            link_color_func=lambda x: link_cols[x],\n",
    "            above_threshold_color=default_color,\n",
    "            distance_sort='descending',\n",
    "            )\n",
    "\n",
    "\n",
    "        self.labels = labels\n",
    "        self.dendogram = dend\n",
    "\n",
    "    # TODO Rename this here and in `clusters`\n",
    "    def _extracted_from_clusters_20(self, threshold):\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        plt.title('Hierarchical Clustering Dendrogram',loc='center',fontsize=40)\n",
    "        plt.xlabel('Data points',fontsize=25)\n",
    "        plt.ylabel('Distance',fontsize=25)\n",
    "        plt.axhline(threshold, color='gray',lw =2, ls  = \"--\")\n",
    "        fig.patch.set_facecolor('white')\n",
    "        #plt.xticks([])\n",
    "        #plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HC = Hierarchical_Clastering()\n",
    "HC.clusters(scaled_dist[::50], 120, no_plot=True) #Thresold is dependent on number of data points,\n",
    "#**HC is the slowest among all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
